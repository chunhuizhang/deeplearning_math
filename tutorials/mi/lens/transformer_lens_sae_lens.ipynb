{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361c8808-c848-4892-8894-3cdb9abd3462",
   "metadata": {},
   "source": [
    "> reverse engineering\n",
    "\n",
    "- https://www.lesswrong.com/posts/hnzHrdqn3nrjveayv/how-to-transformer-mechanistic-interpretability-in-50-lines\n",
    "- https://docs.google.com/presentation/d/1BkAjGqIqgomQpj6j0-oZORCeq72sH7_sOPjlflnYm_A/edit?pli=1#slide=id.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "197be1b3-10df-4e76-8cb0-3a124e8ab8a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:47:52.824020Z",
     "iopub.status.busy": "2024-11-30T13:47:52.823264Z",
     "iopub.status.idle": "2024-11-30T13:47:52.839229Z",
     "shell.execute_reply": "2024-11-30T13:47:52.837936Z",
     "shell.execute_reply.started": "2024-11-30T13:47:52.823977Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adba7c65-bfd6-401d-b63f-bf6774fe7250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:47:52.841604Z",
     "iopub.status.busy": "2024-11-30T13:47:52.840952Z",
     "iopub.status.idle": "2024-11-30T13:47:56.364383Z",
     "shell.execute_reply": "2024-11-30T13:47:56.363564Z",
     "shell.execute_reply.started": "2024-11-30T13:47:52.841563Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from transformer_lens import utils\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698c660-56c3-4754-8b57-7ffd2f78477b",
   "metadata": {},
   "source": [
    "- model cfg\n",
    "    - https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformerConfig.html\n",
    "- `logits, cache = mode.run_witch_cache(dataset)`\n",
    "    - 运行模型时同时捕获中间计算结果（通常是某些层或模块的输出）以便后续分析和调试\n",
    "    - `cache[\"pattern\", layer, \"attn\"]`: 从指定的 layer 层获取注意力模块的注意力模式（权重）。\n",
    "        - 形状为 [batch_size, num_heads, seq_length, seq_length] 的张量，包含注意力权重。\n",
    "    - `cache[\"pre\", layer, \"mlp\"]`: 从指定的 layer 层获取 MLP 模块的预激活输出。\n",
    "        - 形状为 [batch_size, seq_length, d_model] 的张量，包含在激活函数应用之前的值。\n",
    "    - `cache[\"post\", layer, \"mlp\"]`: 从指定的 layer 层获取 MLP 模块的后激活输出。\n",
    "        - 形状为 [batch_size, seq_length, d_model] 的张量，包含在激活函数应用之后的值。\n",
    "- `model.run_with_hooks(..., fwd_hooks=[])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7053d-47fa-4eab-995a-27969f383501",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e0da7-3f9b-4f17-a87e-588d8d7b288a",
   "metadata": {},
   "source": [
    "- Embed()/Unembed()\n",
    "    - Embed: W_E\n",
    "    - Unembed: W_U\n",
    "- MLP()\n",
    "    - W_in\n",
    "    - W_out\n",
    "- Attention()\n",
    "    - QKVO:\n",
    "        - W(weights)\n",
    "            - W_Q, W_K, W_V, W_O\n",
    "        - b(biases)\n",
    "            - b_Q, b_K, b_V, b_O"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54331d-72c1-485e-958c-f95d02cd45ce",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbea209f-57fc-42dd-99fa-51d6f9cf92e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:47:56.365216Z",
     "iopub.status.busy": "2024-11-30T13:47:56.365079Z",
     "iopub.status.idle": "2024-11-30T13:47:56.371783Z",
     "shell.execute_reply": "2024-11-30T13:47:56.371033Z",
     "shell.execute_reply.started": "2024-11-30T13:47:56.365205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blocks.1.mlp.hook_post'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_act_name(\"post\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04540fb6-36c8-47c3-96eb-bf91fd4b2c45",
   "metadata": {},
   "source": [
    "### tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dccd8d-ba00-4cdf-85cf-67d53d5734a8",
   "metadata": {},
   "source": [
    "- model.to_string\n",
    "    - id => string\n",
    "- model.to_str_tokens\n",
    "    - id => token\n",
    "- model.to_single_token\n",
    "    - string => id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef86f975-062b-459e-913b-f3096190028b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:50:31.092916Z",
     "iopub.status.busy": "2024-11-30T13:50:31.092265Z",
     "iopub.status.idle": "2024-11-30T13:50:32.963764Z",
     "shell.execute_reply": "2024-11-30T13:50:32.962126Z",
     "shell.execute_reply.started": "2024-11-30T13:50:31.092869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n",
      "Changing model dtype to torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whaow/anaconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1602: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gelu-1l\").to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d47cef6-cca3-4a3d-bde6-9beeaf70178f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:50:34.514491Z",
     "iopub.status.busy": "2024-11-30T13:50:34.514261Z",
     "iopub.status.idle": "2024-11-30T13:50:34.521736Z",
     "shell.execute_reply": "2024-11-30T13:50:34.520092Z",
     "shell.execute_reply.started": "2024-11-30T13:50:34.514474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48262"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.d_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c33ed3c-63e3-4a9f-94f9-9637c91c15e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:50:36.194655Z",
     "iopub.status.busy": "2024-11-30T13:50:36.194095Z",
     "iopub.status.idle": "2024-11-30T13:50:36.208845Z",
     "shell.execute_reply": "2024-11-30T13:50:36.207084Z",
     "shell.execute_reply.started": "2024-11-30T13:50:36.194612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|EOS|>', '<|BOS|>', '<|PAD|>', '!', '\"', '#', '$', '%', '&', \"'\"]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce82af9-487d-480c-ba1a-59182c152cfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:50:37.800789Z",
     "iopub.status.busy": "2024-11-30T13:50:37.800216Z",
     "iopub.status.idle": "2024-11-30T13:50:37.811344Z",
     "shell.execute_reply": "2024-11-30T13:50:37.809749Z",
     "shell.execute_reply.started": "2024-11-30T13:50:37.800745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_single_token('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62662db1-b3bc-4bff-ab76-c6982cc4eb70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:50:40.374209Z",
     "iopub.status.busy": "2024-11-30T13:50:40.373633Z",
     "iopub.status.idle": "2024-11-30T13:50:40.385853Z",
     "shell.execute_reply": "2024-11-30T13:50:40.383582Z",
     "shell.execute_reply.started": "2024-11-30T13:50:40.374166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(24684)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d0eeac-5d52-4048-bf2d-c200b81fc813",
   "metadata": {},
   "source": [
    "### basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6fa320-ab24-4e28-a437-86032e4ac748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:47:58.945384Z",
     "iopub.status.busy": "2024-11-30T13:47:58.945187Z",
     "iopub.status.idle": "2024-11-30T13:48:01.262946Z",
     "shell.execute_reply": "2024-11-30T13:48:01.261704Z",
     "shell.execute_reply.started": "2024-11-30T13:47:58.945368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Load a model (eg GPT-2 Small)\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c63860d-f03b-4efd-b856-16f6c67baf8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.264068Z",
     "iopub.status.busy": "2024-11-30T13:48:01.263874Z",
     "iopub.status.idle": "2024-11-30T13:48:01.498189Z",
     "shell.execute_reply": "2024-11-30T13:48:01.496458Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.264053Z"
    }
   },
   "outputs": [],
   "source": [
    "logits = model(\"Famous computer scientist Alan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afdcbcd2-74f7-4662-9d57-9dabd7348699",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.499107Z",
     "iopub.status.busy": "2024-11-30T13:48:01.498912Z",
     "iopub.status.idle": "2024-11-30T13:48:01.506440Z",
     "shell.execute_reply": "2024-11-30T13:48:01.505534Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.499090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3938b3-95b0-427f-882f-778ee4500908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.507359Z",
     "iopub.status.busy": "2024-11-30T13:48:01.507175Z",
     "iopub.status.idle": "2024-11-30T13:48:01.521337Z",
     "shell.execute_reply": "2024-11-30T13:48:01.520122Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.507344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Turing\n"
     ]
    }
   ],
   "source": [
    "# The logit dimensions are: [batch, position, vocab]\n",
    "next_token_logits = logits[0, -1]\n",
    "next_token_prediction = next_token_logits.argmax()\n",
    "next_word_prediction = model.tokenizer.decode(next_token_prediction)\n",
    "print(next_word_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3377ed2e-a0be-4d57-9112-fb6c45c4e822",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.522273Z",
     "iopub.status.busy": "2024-11-30T13:48:01.522080Z",
     "iopub.status.idle": "2024-11-30T13:48:01.528683Z",
     "shell.execute_reply": "2024-11-30T13:48:01.527795Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.522258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "838ab0d6-ca58-4f95-a97e-3f907d67ea21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.529693Z",
     "iopub.status.busy": "2024-11-30T13:48:01.529485Z",
     "iopub.status.idle": "2024-11-30T13:48:01.579810Z",
     "shell.execute_reply": "2024-11-30T13:48:01.578525Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.529677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 6, 768])\n",
      "hook_pos_embed torch.Size([1, 6, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.0.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.1.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.1.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.1.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.1.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.1.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.1.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.1.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.1.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.1.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.1.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.1.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.1.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.1.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.1.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.1.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.1.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.1.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.2.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.2.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.2.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.2.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.2.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.2.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.2.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.2.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.2.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.2.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.2.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.2.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.2.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.2.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.2.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.2.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.2.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.3.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.3.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.3.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.3.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.3.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.3.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.3.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.3.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.3.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.3.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.3.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.3.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.3.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.3.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.3.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.3.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.3.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.4.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.4.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.4.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.4.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.4.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.4.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.4.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.4.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.4.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.4.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.4.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.4.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.4.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.4.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.4.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.4.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.4.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.5.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.5.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.5.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.5.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.5.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.5.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.5.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.5.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.5.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.5.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.5.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.5.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.5.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.5.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.5.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.5.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.5.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.6.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.6.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.6.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.6.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.6.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.6.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.6.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.6.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.6.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.6.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.6.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.6.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.6.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.6.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.6.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.6.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.6.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.7.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.7.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.7.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.7.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.7.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.7.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.7.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.7.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.7.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.7.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.7.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.7.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.7.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.7.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.7.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.7.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.7.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.8.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.8.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.8.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.8.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.8.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.8.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.8.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.8.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.8.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.8.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.8.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.8.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.8.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.8.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.8.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.8.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.8.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.9.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.9.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.9.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.9.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.9.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.9.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.9.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.9.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.9.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.9.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.9.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.9.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.9.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.9.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.9.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.9.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.9.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.10.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.10.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.10.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.10.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.10.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.10.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.10.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.10.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.10.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.10.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.10.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.10.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.10.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.10.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.10.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.10.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.10.hook_resid_post torch.Size([1, 6, 768])\n",
      "blocks.11.hook_resid_pre torch.Size([1, 6, 768])\n",
      "blocks.11.ln1.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.11.ln1.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.11.attn.hook_q torch.Size([1, 6, 12, 64])\n",
      "blocks.11.attn.hook_k torch.Size([1, 6, 12, 64])\n",
      "blocks.11.attn.hook_v torch.Size([1, 6, 12, 64])\n",
      "blocks.11.attn.hook_attn_scores torch.Size([1, 12, 6, 6])\n",
      "blocks.11.attn.hook_pattern torch.Size([1, 12, 6, 6])\n",
      "blocks.11.attn.hook_z torch.Size([1, 6, 12, 64])\n",
      "blocks.11.hook_attn_out torch.Size([1, 6, 768])\n",
      "blocks.11.hook_resid_mid torch.Size([1, 6, 768])\n",
      "blocks.11.ln2.hook_scale torch.Size([1, 6, 1])\n",
      "blocks.11.ln2.hook_normalized torch.Size([1, 6, 768])\n",
      "blocks.11.mlp.hook_pre torch.Size([1, 6, 3072])\n",
      "blocks.11.mlp.hook_post torch.Size([1, 6, 3072])\n",
      "blocks.11.hook_mlp_out torch.Size([1, 6, 768])\n",
      "blocks.11.hook_resid_post torch.Size([1, 6, 768])\n",
      "ln_final.hook_scale torch.Size([1, 6, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = model.run_with_cache(\"Famous computer scientist Alan\")\n",
    "for key, value in cache.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8ed97f-eda5-4b7f-8cff-a7076a4a0995",
   "metadata": {},
   "source": [
    "- 关于 `hook_pattern`、`hook_z`\n",
    "    - `pattern = softmax((Q @ K.T) / sqrt(d_k))  # [batch, seq_len, seq_len]`\n",
    "    - `Z = pattern @ V  # weighted sum of value vectors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7051bb-1c96-414a-af96-3fb0ab3c812a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.580773Z",
     "iopub.status.busy": "2024-11-30T13:48:01.580578Z",
     "iopub.status.idle": "2024-11-30T13:48:01.628195Z",
     "shell.execute_reply": "2024-11-30T13:48:01.626580Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.580758Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the model and get logits and activations\n",
    "logits, cache = model.run_with_cache(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21b89389-6e4d-4997-bd8e-58efd8ef2b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.629277Z",
     "iopub.status.busy": "2024-11-30T13:48:01.629063Z",
     "iopub.status.idle": "2024-11-30T13:48:01.635566Z",
     "shell.execute_reply": "2024-11-30T13:48:01.634666Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.629261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50257])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cec533b6-e675-4463-bd68-5e6e4c1a97a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.636665Z",
     "iopub.status.busy": "2024-11-30T13:48:01.636460Z",
     "iopub.status.idle": "2024-11-30T13:48:01.644266Z",
     "shell.execute_reply": "2024-11-30T13:48:01.643389Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.636650Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 3, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[\"blocks.0.attn.hook_pattern\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "040e784e-c360-41cf-b59c-eb8f9145f7a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.645255Z",
     "iopub.status.busy": "2024-11-30T13:48:01.645056Z",
     "iopub.status.idle": "2024-11-30T13:48:01.727510Z",
     "shell.execute_reply": "2024-11-30T13:48:01.726088Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.645239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0623, -0.2089, -0.0441,  0.7658, -0.0960, -0.4061, -0.0387, -0.1681,\n",
       "        -0.1954,  0.2037, -0.1689, -0.0635,  0.0478,  0.0231, -0.2398,  0.1860,\n",
       "        -0.2425, -0.0643, -0.0097, -0.0837,  0.1259, -0.0514, -0.0949, -0.1342,\n",
       "        -0.1288,  0.1212,  0.0954,  0.2853,  0.0163, -0.1749,  0.1404, -0.0348,\n",
       "        -0.3939,  0.2363, -0.0986,  0.0756,  0.3388,  0.0141, -0.0032, -0.0519,\n",
       "         0.0807, -0.0332,  0.1161,  0.2470, -0.1805, -0.0772, -0.8262,  0.2272,\n",
       "        -0.0716,  0.1921, -0.2517, -0.0693, -0.3242,  0.0707, -0.4548,  0.0135,\n",
       "         0.0316,  0.0898, -0.0733,  0.2011,  0.1891,  0.2649, -0.2121, -0.1774],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_idx = 5\n",
    "pos = 1\n",
    "weighting = cache[\"blocks.0.attn.hook_pattern\"][0, head_idx, pos, :] \n",
    "v = cache[\"blocks.0.attn.hook_v\"][0, :, head_idx, :]\n",
    "z = weighting @ v  # 等同于 cache[\"blocks.0.attn.hook_z\"][0, pos, head_idx, :]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac7a877-9d07-4cb7-aeea-b2dd5af6ad28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.728851Z",
     "iopub.status.busy": "2024-11-30T13:48:01.728628Z",
     "iopub.status.idle": "2024-11-30T13:48:01.736714Z",
     "shell.execute_reply": "2024-11-30T13:48:01.735809Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.728833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0623, -0.2089, -0.0441,  0.7658, -0.0960, -0.4061, -0.0387, -0.1681,\n",
       "        -0.1954,  0.2037, -0.1689, -0.0635,  0.0478,  0.0231, -0.2398,  0.1860,\n",
       "        -0.2425, -0.0643, -0.0097, -0.0837,  0.1259, -0.0514, -0.0949, -0.1342,\n",
       "        -0.1288,  0.1212,  0.0954,  0.2853,  0.0163, -0.1749,  0.1404, -0.0348,\n",
       "        -0.3939,  0.2363, -0.0986,  0.0756,  0.3388,  0.0141, -0.0032, -0.0519,\n",
       "         0.0807, -0.0332,  0.1161,  0.2470, -0.1805, -0.0772, -0.8262,  0.2272,\n",
       "        -0.0716,  0.1921, -0.2517, -0.0693, -0.3242,  0.0707, -0.4548,  0.0135,\n",
       "         0.0316,  0.0898, -0.0733,  0.2011,  0.1891,  0.2649, -0.2121, -0.1774],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache[\"blocks.0.attn.hook_z\"][0, pos, head_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc6c39c-61db-447c-84c7-e7b136110396",
   "metadata": {},
   "source": [
    "### induction heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4446977c-bdd4-4252-b6b7-5e2091cd2095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.737885Z",
     "iopub.status.busy": "2024-11-30T13:48:01.737674Z",
     "iopub.status.idle": "2024-11-30T13:48:01.834214Z",
     "shell.execute_reply": "2024-11-30T13:48:01.833288Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.737867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Her', ' name', ' was', ' Alex', ' Hart', '.', ' Tomorrow', ' at', ' lunch', ' time', ' Alex']\n",
      "Tokenized answer: [' Hart']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.29</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7.35</span><span style=\"font-weight: bold\">% Token: | Hart|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m14.29\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m7.35\u001b[0m\u001b[1m% Token: | Hart|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 15.64 Prob: 28.38% Token: | will|\n",
      "Top 1th token. Logit: 14.47 Prob:  8.79% Token: | would|\n",
      "Top 2th token. Logit: 14.34 Prob:  7.74% Token: | was|\n",
      "Top 3th token. Logit: 14.29 Prob:  7.35% Token: | Hart|\n",
      "Top 4th token. Logit: 14.18 Prob:  6.54% Token: | and|\n",
      "Top 5th token. Logit: 14.09 Prob:  6.00% Token: | is|\n",
      "Top 6th token. Logit: 13.51 Prob:  3.38% Token: |'s|\n",
      "Top 7th token. Logit: 13.23 Prob:  2.53% Token: |,|\n",
      "Top 8th token. Logit: 12.73 Prob:  1.55% Token: | had|\n",
      "Top 9th token. Logit: 12.00 Prob:  0.74% Token: | has|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Hart'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Hart'\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(\"Her name was Alex Hart. Tomorrow at lunch time Alex\",\n",
    "                  answer=\" Hart\", model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fef9ef-8913-4e72-8f36-1ef8ee7b3dda",
   "metadata": {},
   "source": [
    "### hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6eb42b1-4980-4b3b-b8ac-964b96935387",
   "metadata": {},
   "source": [
    "- 在深度学习框架（如 PyTorch）中，hook 是一种可以在模型的**前向或后向传播**过程中插入自定义函数的机制。这种机制允许我们在网络的计算过程中，**访问或修改中间的激活值**，从而深入理解模型的内部工作原理。\n",
    "    - 前向钩子（Forward Hook）：在模块的 forward 方法执行时，hook 会在输入被传递到模块或输出被返回之前或之后被调用。它可以访问并修改输入和输出的张量。\n",
    "    - 后向钩子（Backward Hook）：在反向传播过程中，hook 可以访问和修改梯度。\n",
    "- transformer_lens 是一个用于分析和解释 Transformer 模型的库。它通过扩展基础的 Transformer 模型，引入了可以在模型的各个层次上添加 hook 的功能。这使得研究者可以：\n",
    "    - 记录激活值：在模型的特定层次或位置，提取中间激活值以进行分析。例如，提取注意力权重、隐藏状态等。\n",
    "    - 修改激活值：在前向传播过程中，修改中间激活值以测试模型对某些干预的响应。这对于理解模型如何处理信息非常有用。\n",
    "    - 消融实验：通过零化某些神经元的激活值，观察模型输出的变化，以确定这些神经元在特定任务中的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fef6c8a0-d4b5-47b6-9327-ff03bc4eeb00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.835828Z",
     "iopub.status.busy": "2024-11-30T13:48:01.835412Z",
     "iopub.status.idle": "2024-11-30T13:48:01.839731Z",
     "shell.execute_reply": "2024-11-30T13:48:01.838817Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.835809Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.hook_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4571a79a-9ab8-4bba-858d-9fdfdc80dc67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.842915Z",
     "iopub.status.busy": "2024-11-30T13:48:01.842697Z",
     "iopub.status.idle": "2024-11-30T13:48:01.848922Z",
     "shell.execute_reply": "2024-11-30T13:48:01.848050Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.842899Z"
    }
   },
   "outputs": [],
   "source": [
    "# def attention_pattern_hook(activation, hook):\n",
    "#     hook.ctx[\"pattern\"] = activation.detach().clone()\n",
    "#     print(f\"Attention pattern shape at {hook.name}: {activation.shape}\")\n",
    "#     return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78864223-1ab6-409b-babe-cae4e275dfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.849995Z",
     "iopub.status.busy": "2024-11-30T13:48:01.849804Z",
     "iopub.status.idle": "2024-11-30T13:48:01.857750Z",
     "shell.execute_reply": "2024-11-30T13:48:01.856849Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.849980Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 假设我们想在模型的某一层添加钩子，比如第一层的注意力输出\n",
    "# hook_name = 'blocks.0.attn.hook_pattern'  # 第一层注意力层的输出钩子\n",
    "\n",
    "# # 在指定的层添加前向钩子\n",
    "# model.add_hook(hook_name, attention_pattern_hook, dir='fwd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70b24aaa-b1ac-4fee-8fa8-ae37123b31e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.858680Z",
     "iopub.status.busy": "2024-11-30T13:48:01.858480Z",
     "iopub.status.idle": "2024-11-30T13:48:01.871777Z",
     "shell.execute_reply": "2024-11-30T13:48:01.869875Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.858665Z"
    }
   },
   "outputs": [],
   "source": [
    "# logits = model(\"Famous computer scientist Alan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9201e111-1107-4be0-80c7-5ac243873673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.873161Z",
     "iopub.status.busy": "2024-11-30T13:48:01.872915Z",
     "iopub.status.idle": "2024-11-30T13:48:01.879992Z",
     "shell.execute_reply": "2024-11-30T13:48:01.879000Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.873141Z"
    }
   },
   "outputs": [],
   "source": [
    "# def extended_attention_hook(activation, hook):\n",
    "#     \"\"\"Extended version with more analysis\"\"\"\n",
    "#     pattern = activation.detach()\n",
    "    \n",
    "#     # Get sequence length\n",
    "#     seq_len = pattern.shape[-1]\n",
    "    \n",
    "#     # Calculate average attention per position\n",
    "#     avg_attention = pattern.mean(dim=(0,1))  # Average across batch and heads\n",
    "    \n",
    "#     # Find which tokens get most attention\n",
    "#     max_attended_pos = avg_attention.argmax(dim=-1)\n",
    "    \n",
    "#     print(f\"\\nAnalysis for {hook.name}:\")\n",
    "#     print(f\"- Shape: {pattern.shape}\")\n",
    "#     print(f\"- Most attended position: {max_attended_pos.tolist()}\")\n",
    "    \n",
    "#     # Store for later use\n",
    "#     hook.ctx[\"pattern\"] = pattern\n",
    "#     hook.ctx[\"avg_attention\"] = avg_attention\n",
    "    \n",
    "#     return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "701162e7-b468-4e85-83d2-3b023fc17c53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.881471Z",
     "iopub.status.busy": "2024-11-30T13:48:01.881206Z",
     "iopub.status.idle": "2024-11-30T13:48:01.888872Z",
     "shell.execute_reply": "2024-11-30T13:48:01.887841Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.881449Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Hook specific layers\n",
    "# model.blocks[0].attn.hook_pattern.add_hook(extended_attention_hook)  # First layer\n",
    "# model.blocks[-1].attn.hook_pattern.add_hook(extended_attention_hook) # Last layer\n",
    "\n",
    "# # Hook multiple components\n",
    "# hooks = []\n",
    "# for layer in range(model.cfg.n_layers):\n",
    "#     hooks.append(\n",
    "#         model.blocks[layer].attn.hook_pattern.add_hook(\n",
    "#             extended_attention_hook,\n",
    "#             # name=f\"attn_pattern_layer_{layer}\"\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e196a2ee-02cb-4ef1-aa38-7c9f66f16349",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-30T13:48:01.890486Z",
     "iopub.status.busy": "2024-11-30T13:48:01.890027Z",
     "iopub.status.idle": "2024-11-30T13:48:01.897539Z",
     "shell.execute_reply": "2024-11-30T13:48:01.896525Z",
     "shell.execute_reply.started": "2024-11-30T13:48:01.890461Z"
    }
   },
   "outputs": [],
   "source": [
    "# logits = model(\"Famous computer scientist Alan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
