{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2698c660-56c3-4754-8b57-7ffd2f78477b",
   "metadata": {},
   "source": [
    "- model cfg\n",
    "    - https://transformerlensorg.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformerConfig.html\n",
    "- `logits, cache = mode.run_witch_cache(dataset)`\n",
    "    - 运行模型时同时捕获中间计算结果（通常是某些层或模块的输出）以便后续分析和调试\n",
    "    - `cache[\"pattern\", layer, \"attn\"]`: 从指定的 layer 层获取注意力模块的注意力模式（权重）。\n",
    "        - 形状为 [batch_size, num_heads, seq_length, seq_length] 的张量，包含注意力权重。\n",
    "    - `cache[\"pre\", layer, \"mlp\"]`: 从指定的 layer 层获取 MLP 模块的预激活输出。\n",
    "        - 形状为 [batch_size, seq_length, d_model] 的张量，包含在激活函数应用之前的值。\n",
    "    - `cache[\"post\", layer, \"mlp\"]`: 从指定的 layer 层获取 MLP 模块的后激活输出。\n",
    "        - 形状为 [batch_size, seq_length, d_model] 的张量，包含在激活函数应用之后的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7053d-47fa-4eab-995a-27969f383501",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e0da7-3f9b-4f17-a87e-588d8d7b288a",
   "metadata": {},
   "source": [
    "- Embed()/Unembed()\n",
    "    - Embed: W_E\n",
    "    - Unembed: W_U\n",
    "- MLP()\n",
    "    - W_in\n",
    "    - W_out\n",
    "- Attention()\n",
    "    - QKVO:\n",
    "        - W(weights)\n",
    "            - W_Q, W_K, W_V, W_O\n",
    "        - b(biases)\n",
    "            - b_Q, b_K, b_V, b_O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
