{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3074ed43-6d90-492f-88b0-442602dc42d6",
   "metadata": {},
   "source": [
    "- the updates produced by both SGD-momentum and Adam for the 2D parameters in transformer-based neural networks typically have very high **condition number**. That is, they are almost **low-rank matrices**, with the updates for all neurons being dominated by just a few directions.\n",
    "    - https://kellerjordan.github.io/posts/muon/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90838a5-5836-4956-8b94-9541988244e1",
   "metadata": {},
   "source": [
    "- 矩阵的条件数（Condition Number）衡量的是当输入数据发生微小变化时，输出结果会发生多大程度的变化。换句话说，它是一个数值计算稳定性的指标。\n",
    "- 对于一个方阵 $A$，其条件数 $\\kappa(A)$ 通用定义\n",
    "    - $\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|$\n",
    "    - 这个定义已经暗示了，只有可逆矩阵（非奇异矩阵）才有（有限的）条件数。如果一个矩阵是奇异的（不可逆），那么 $A^{-1}$ 不存在，或者是无穷大的；\n",
    "- 实际计算中，最常用和最能揭示几何本质的是基于奇异值分解 (SVD) 的计算方法。\n",
    "    - 假设 $A$ 的的奇异值从大到小排列为 $\\sigma_1, \\sigma_2, \\cdots, \\sigma_n$\n",
    "    - $\\kappa(A) = \\frac{\\sigma_{\\text{max}}}{\\sigma_{\\text{min}}}$\n",
    "- 纯数值计算的角度来看，一个训练好的神经网络的权重矩阵或其梯度矩阵，用 np.linalg.matrix_rank 来计算，几乎 100% 会是满秩的。\n",
    "    - 随机初始化：网络权重在训练开始时就是用小的随机数（例如，从高斯分布或均匀分布中采样）初始化的。从数学概率上讲，随机生成的矩阵几乎不可能存在精确的线性相关性。\n",
    "    - 随机梯度下降 (SGD) 的更新：在训练过程中，每次参数更新都是一个微小的、带有噪声的浮点数向量。这些连续、随机的扰动会不断地打破任何可能形成的线性依赖关系，使得矩阵中的每一行/列都保持着微小的独特性。\n",
    "    - 数值秩 (Numerical Rank) vs. 有效秩 (Effective Rank)\n",
    "        - 你用 np.linalg.matrix_rank 得到的是数值秩。这个函数计算矩阵的奇异值，然后统计有多少个奇异值大于一个非常小的阈值（tolerance）。由于浮点数的精度，这些奇异值几乎都不会精确为零。\n",
    "        - 更关心的是有效秩 (Effective Rank) 或称 内在秩 (Intrinsic Rank)。这个概念关注的是：有多少个奇异值是“显著的”或“大的”，而其他的都可以被忽略不计？\n",
    "        - 一个权重矩阵的奇异值谱（Singular Value Spectrum），也就是将其所有奇异值从大到小画出来。对于神经网络的权重矩阵，这个谱通常呈现以下特征：\n",
    "            - 急剧下降 (Sharp Decay)：有少数几个非常大的奇异值。\n",
    "            - 长长的尾巴 (Long Tail)：后面跟着大量非常非常小、但非零的奇异值。\n",
    "    - 为什么会出现“有效低秩”现象？(The Low-Rank Hypothesis)\n",
    "        - 过度参数化 (Over-parameterization)：现代神经网络的参数数量远远超过训练数据的数量。例如，一个GPT模型有数十亿参数，但训练数据量远小于此。这意味着网络有巨大的“表达自由度”，它不需要利用矩阵的全部“容量”（即满秩）来拟合数据。它可以在这个巨大的参数空间中找到一个更“简单”的、本质上是低秩的解。\n",
    "        - 信息的内在维度 (Intrinsic Dimension of Information)：网络学习的任务通常具有比输入数据维度低得多的内在结构。例如，要识别一张 256x256 像素图片里的猫，虽然输入维度是 65536，但“猫”这个概念的内在信息维度要低得多。网络层学会的是将数据投影到一个低维子空间（manifold）上，在这个子空间里，分类或回归任务变得更容易。这个“投影”操作，其本质就是低秩的。\n",
    "        - SGD的隐式正则化 (Implicit Regularization of SGD)：研究表明，随机梯度下降本身就有一种“偏好”，它倾向于寻找那些泛化能力更好的解，而这些解往往对应于更“平坦”的损失函数区域和更简单的（例如，低秩的）参数配置。\n",
    "    - 一个具有“有效低秩”特性的矩阵，必然是一个病态矩阵 (Ill-conditioned Matrix)，它的条件数会极其巨大。\n",
    "        - $\\kappa(A) = \\frac{\\sigma_{\\text{max}}}{\\sigma_{\\text{min}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087a5e3d-63b0-4b7d-ac41-14b16ba5b86a",
   "metadata": {},
   "source": [
    "- 这个“有效低秩”的发现不是一个纯理论游戏，它催生了AI领域一些最重要的技术：\n",
    "    - 模型压缩 (Model Compression)：既然矩阵是有效低秩的，我们就可以用一个真正的低秩矩阵来近似它，从而大幅减少参数量。例如，将一个大矩阵 $W_{m \\times n}$ 分解为两个小矩阵 $A_{m \\times k}$ 和 $B_{k \\times n}$的乘积（其中 $k \\ll m, n$），参数量则由 $m\\times n$ 锐减为 $k\\times (m+n)$\n",
    "    - LoRA (Low-Rank Adaptation)：这是目前在大型语言模型微调中最火的技术之一。它的核心思想是：在微调时，原始的巨大权重矩阵 $W_0$ 保持不变（冻结），我们只训练一个非常小的、低秩的“更新矩阵” $\\Delta W = BA$，。最终的权重是 $W = W_0 + BA$。这极大地降低了微调的计算和存储成本。LoRA的成功，就是“有效低秩假设”最有力的证明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29def53c-e064-409b-8501-546ed7216ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Matrix: Well-Conditioned Matrix ---\n",
      "Matrix A:\n",
      " [[1. 0.]\n",
      " [0. 1.]]\n",
      "\n",
      "Rank of A: 2\n",
      "Singular values (s): [1. 1.]\n",
      "  - Max singular value (σ_max): 1.000000\n",
      "  - Min singular value (σ_min): 1.000000\n",
      "\n",
      "Condition number (from np.linalg.cond): 1.00\n",
      "Condition number (from σ_max / σ_min): 1.00\n",
      "\n",
      "Conclusion: This is a well-conditioned matrix.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Matrix: Ill-Conditioned Matrix ---\n",
      "Matrix A:\n",
      " [[1.       1.      ]\n",
      " [1.       1.000001]]\n",
      "\n",
      "Rank of A: 2\n",
      "Singular values (s): [2.00000050e+00 4.99999875e-07]\n",
      "  - Max singular value (σ_max): 2.000001\n",
      "  - Min singular value (σ_min): 0.000000\n",
      "\n",
      "Condition number (from np.linalg.cond): 4,000,002.00\n",
      "Condition number (from σ_max / σ_min): 4,000,002.00\n",
      "\n",
      "Conclusion: This is an ill-conditioned (sick) matrix.\n",
      "It is very close to being a singular (lower rank) matrix because its smallest singular value is tiny compared to its largest.\n",
      "----------------------------------------\n",
      "\n",
      "--- Analyzing Matrix: Singular Matrix ---\n",
      "Matrix A:\n",
      " [[1. 2.]\n",
      " [2. 4.]]\n",
      "\n",
      "Rank of A: 1\n",
      "Singular values (s): [5.00000000e+00 1.04061363e-16]\n",
      "  - Max singular value (σ_max): 5.000000\n",
      "  - Min singular value (σ_min): 0.000000\n",
      "\n",
      "Condition number (from np.linalg.cond): inf\n",
      "Condition number (from σ_max / σ_min): inf\n",
      "\n",
      "Conclusion: This is an ill-conditioned (sick) matrix.\n",
      "It is very close to being a singular (lower rank) matrix because its smallest singular value is tiny compared to its largest.\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def analyze_matrix(name, A):\n",
    "    \"\"\"\n",
    "    Analyzes a matrix to show its rank, singular values, and condition number.\n",
    "    \"\"\"\n",
    "    print(f\"--- Analyzing Matrix: {name} ---\")\n",
    "    print(\"Matrix A:\\n\", A)\n",
    "\n",
    "    # Calculate rank\n",
    "    rank = np.linalg.matrix_rank(A)\n",
    "    print(f\"\\nRank of A: {rank}\")\n",
    "\n",
    "    # Calculate singular values\n",
    "    # The svd function returns U, s, Vh. 's' contains the singular values.\n",
    "    U, s, Vh = np.linalg.svd(A)\n",
    "    print(f\"Singular values (s): {s}\")\n",
    "    \n",
    "    sigma_max = np.max(s)\n",
    "    sigma_min = np.min(s)\n",
    "    print(f\"  - Max singular value (σ_max): {sigma_max:.6f}\")\n",
    "    print(f\"  - Min singular value (σ_min): {sigma_min:.6f}\")\n",
    "\n",
    "    # Calculate condition number\n",
    "    # Check if the matrix is singular before calculating condition number\n",
    "    if sigma_min < 1e-15: # A small threshold to check for numerical zero\n",
    "        cond_num = float('inf')\n",
    "        cond_num_manual = float('inf')\n",
    "    else:\n",
    "        cond_num = np.linalg.cond(A)\n",
    "        cond_num_manual = sigma_max / sigma_min\n",
    "\n",
    "    print(f\"\\nCondition number (from np.linalg.cond): {cond_num:,.2f}\")\n",
    "    print(f\"Condition number (from σ_max / σ_min): {cond_num_manual:,.2f}\")\n",
    "    \n",
    "    if cond_num > 1000:\n",
    "        print(\"\\nConclusion: This is an ill-conditioned (sick) matrix.\")\n",
    "        print(\"It is very close to being a singular (lower rank) matrix because its smallest singular value is tiny compared to its largest.\")\n",
    "    elif cond_num == float('inf'):\n",
    "        print(\"\\nConclusion: This is a singular matrix (rank-deficient). Its condition number is infinite.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: This is a well-conditioned matrix.\")\n",
    "    print(\"-\" * 40 + \"\\n\")\n",
    "\n",
    "\n",
    "# 1. A well-conditioned matrix (close to identity)\n",
    "# The two column vectors are orthogonal.\n",
    "A_well = np.array([[1.0, 0.0],\n",
    "                   [0.0, 1.0]])\n",
    "analyze_matrix(\"Well-Conditioned Matrix\", A_well)\n",
    "\n",
    "\n",
    "# 2. An ill-conditioned matrix\n",
    "# The second column is very close to being a multiple of the first.\n",
    "# It's \"almost\" a rank-1 matrix.\n",
    "A_ill = np.array([[1.0, 1.0],\n",
    "                  [1.0, 1.000001]])\n",
    "analyze_matrix(\"Ill-Conditioned Matrix\", A_ill)\n",
    "\n",
    "\n",
    "# 3. A singular matrix (rank-deficient)\n",
    "# The second column is exactly a multiple of the first.\n",
    "A_singular = np.array([[1.0, 2.0],\n",
    "                       [2.0, 4.0]])\n",
    "analyze_matrix(\"Singular Matrix\", A_singular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a6e09-07fa-428c-ade5-780aabd31f87",
   "metadata": {},
   "source": [
    "### gpt2 analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e604c9e-bfea-467b-a014-6ffdd1bcf921",
   "metadata": {},
   "source": [
    "- 一组矩阵极度病态 (Ill-Conditioned)：几乎所有 Attention Proj 层的条件数都非常高，甚至有几个在数值上直接被判断为降秩 (Rank-Deficient)。\n",
    "- 另一组矩阵非常健康 (Well-Conditioned)：几乎所有的 Attention (Q,K,V Fused)、MLP FC (Up-Proj) 和 MLP Proj (Down-Proj) 层的条件数都出奇地小（大多在10到200之间），是典型的良态矩阵。\n",
    "    - `block.attn.c_attn.weight`: `[768, 2304]`\n",
    "        - W_q, W_k, W_v\n",
    "- attention\n",
    "    - $ \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) = \\text{softmax}\\left(\\frac{Q_i K_i^T}{\\sqrt{d_k}}\\right)V_i $\n",
    "    - $ \\text{MultiHeadOutput} = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h) $\n",
    "    - $ \\text{Output}_{\\text{Attn}} = \\text{MultiHeadOutput} \\cdot W_o + b_o $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a58398-0754-49db-9baa-e6aba83f0c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhangchunhui/miniconda3/envs/verl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Model\n",
    "import numpy as np\n",
    "\n",
    "def analyze_matrix(name: str, matrix: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Analyzes a given matrix (PyTorch tensor) to compute and print its\n",
    "    shape, numerical rank, and condition number.\n",
    "    \"\"\"\n",
    "    # Ensure matrix is 2D and float for linalg operations\n",
    "    if matrix.dim() > 2:\n",
    "        # For layers like c_attn which are (embedding_dim, 3 * embedding_dim),\n",
    "        # we can analyze them directly. This is a common fused matrix.\n",
    "        pass\n",
    "    elif matrix.dim() < 2:\n",
    "        print(f\"--- Skipping {name}: Not a 2D matrix (shape: {matrix.shape}) ---\\n\")\n",
    "        return\n",
    "\n",
    "    # Move to float32 for accurate linalg calculations\n",
    "    matrix = matrix.to(torch.float32)\n",
    "    \n",
    "    print(f\"--- Analyzing: {name} ---\")\n",
    "    print(f\"Shape: {matrix.shape}\")\n",
    "\n",
    "    # --- Rank Calculation ---\n",
    "    # torch.linalg.matrix_rank uses SVD and counts singular values > tol\n",
    "    rank = torch.linalg.matrix_rank(matrix)\n",
    "    max_possible_rank = min(matrix.shape)\n",
    "    print(f\"Numerical Rank: {rank.item()} (Max possible: {max_possible_rank})\")\n",
    "    if rank.item() == max_possible_rank:\n",
    "        print(\"-> The matrix is numerically FULL RANK.\")\n",
    "    else:\n",
    "        print(\"-> The matrix is numerically RANK-DEFICIENT.\")\n",
    "\n",
    "    # --- Condition Number Calculation ---\n",
    "    # torch.linalg.cond also uses SVD: sigma_max / sigma_min\n",
    "    # This can be very slow for large matrices.\n",
    "    print(\"Calculating condition number (this may take a moment)...\")\n",
    "    try:\n",
    "        # For non-square matrices, cond is still calculated via singular values\n",
    "        cond_num = torch.linalg.cond(matrix)\n",
    "        print(f\"Condition Number: {cond_num.item():.2e}\") # Use scientific notation\n",
    "        if cond_num > 1e5:\n",
    "             print(\"-> The matrix is EXTREMELY ILL-CONDITIONED.\")\n",
    "        elif cond_num > 1e3:\n",
    "             print(\"-> The matrix is ill-conditioned.\")\n",
    "        else:\n",
    "             print(\"-> The matrix is well-conditioned.\")\n",
    "    except torch.linalg.LinAlgError as e:\n",
    "        print(f\"Could not compute condition number: {e}\")\n",
    "    \n",
    "    print(\"-\" * (len(name) + 18) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17d9d0c3-a3d1-4283-b4a3-74df356f4266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading pre-trained gpt2 model...\n",
      "=========================================\n",
      " GPT-2 Transformer Block 0 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 0 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.55e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 0 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 765 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.09e+05\n",
      "-> The matrix is EXTREMELY ILL-CONDITIONED.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 0 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.44e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 0 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.99e+01\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 1 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 1 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.13e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 1 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 766 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.35e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 1 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 5.24e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 1 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.13e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 2 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 2 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 5.03e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 2 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.04e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 2 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.01e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 2 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.89e+01\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 3 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 3 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.36e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 3 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.76e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 3 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 9.08e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 3 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.85e+01\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 4 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 4 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.84e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 4 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.21e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 4 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.51e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 4 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.13e+01\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 5 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 5 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 3.27e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 5 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.11e+03\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 5 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 7.85e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 5 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.10e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 6 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 6 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.29e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 6 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.36e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 6 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 7.16e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 6 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.75e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 7 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 7 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.22e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 7 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.15e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 7 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.16e+02\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 7 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.45e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 8 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 8 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 9.72e+00\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 8 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 4.73e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 8 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.90e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 8 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 3.47e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 9 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 9 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.10e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 9 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 767 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 1.22e+05\n",
      "-> The matrix is EXTREMELY ILL-CONDITIONED.\n",
      "------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 9 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.69e+01\n",
      "-> The matrix is well-conditioned.\n",
      "--------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 9 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.92e+02\n",
      "-> The matrix is well-conditioned.\n",
      "------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 10 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 10 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 9.73e+00\n",
      "-> The matrix is well-conditioned.\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 10 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 766 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.47e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "-------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 10 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 6.29e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 10 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 6.43e+01\n",
      "-> The matrix is well-conditioned.\n",
      "-------------------------------------------------\n",
      "\n",
      "=========================================\n",
      " GPT-2 Transformer Block 11 \n",
      "=========================================\n",
      "\n",
      "--- Analyzing: Block 11 - Attention (Q,K,V Fused) ---\n",
      "Shape: torch.Size([768, 2304])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.14e+01\n",
      "-> The matrix is well-conditioned.\n",
      "----------------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 11 - Attention Proj ---\n",
      "Shape: torch.Size([768, 768])\n",
      "Numerical Rank: 766 (Max possible: 768)\n",
      "-> The matrix is numerically RANK-DEFICIENT.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 2.69e+04\n",
      "-> The matrix is ill-conditioned.\n",
      "-------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 11 - MLP FC (Up-Proj) ---\n",
      "Shape: torch.Size([768, 3072])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 8.25e+01\n",
      "-> The matrix is well-conditioned.\n",
      "---------------------------------------------\n",
      "\n",
      "--- Analyzing: Block 11 - MLP Proj (Down-Proj) ---\n",
      "Shape: torch.Size([3072, 768])\n",
      "Numerical Rank: 768 (Max possible: 768)\n",
      "-> The matrix is numerically FULL RANK.\n",
      "Calculating condition number (this may take a moment)...\n",
      "Condition Number: 3.85e+01\n",
      "-> The matrix is well-conditioned.\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Load pre-trained GPT-2 model from Hugging Face\n",
    "print(\"Loading pre-trained gpt2 model...\")\n",
    "# We use GPT2Model to get the core transformer blocks without the language model head\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "# We don't need to compute gradients for this analysis\n",
    "with torch.no_grad():\n",
    "    # Iterate through each transformer block (layer)\n",
    "    for i, block in enumerate(model.h):\n",
    "        print(f\"=========================================\")\n",
    "        print(f\" GPT-2 Transformer Block {i} \")\n",
    "        print(f\"=========================================\\n\")\n",
    "\n",
    "        # 1. Attention Layer Analysis\n",
    "        # In GPT-2, Q, K, V are combined into one large matrix `c_attn`\n",
    "        attn_weights = block.attn.c_attn.weight\n",
    "        analyze_matrix(f\"Block {i} - Attention (Q,K,V Fused)\", attn_weights)\n",
    "        \n",
    "        # Also analyze the output projection of the attention layer\n",
    "        attn_proj_weights = block.attn.c_proj.weight\n",
    "        analyze_matrix(f\"Block {i} - Attention Proj\", attn_proj_weights)\n",
    "\n",
    "        # 2. MLP (Feed-Forward) Layer Analysis\n",
    "        # First fully-connected layer (up-projection)\n",
    "        mlp_fc_weights = block.mlp.c_fc.weight\n",
    "        analyze_matrix(f\"Block {i} - MLP FC (Up-Proj)\", mlp_fc_weights)\n",
    "\n",
    "        # Second fully-connected layer (down-projection)\n",
    "        mlp_proj_weights = block.mlp.c_proj.weight\n",
    "        analyze_matrix(f\"Block {i} - MLP Proj (Down-Proj)\", mlp_proj_weights)\n",
    "        \n",
    "        # Let's just analyze the first few layers to keep the output concise\n",
    "        # if i == 1:\n",
    "        #     print(\"... analysis shown for the first 2 blocks ...\")\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af30b28-a632-436c-af83-ecb44c3f5b53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "verl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
