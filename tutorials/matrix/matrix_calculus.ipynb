{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912d027e-a47e-406b-af75-49273d70987e",
   "metadata": {},
   "source": [
    "> 所有的深度学习框架（torch、tf、mxnet 等）都是对矩阵微分的封装；\n",
    "\n",
    "- references\n",
    "    - https://atmos.washington.edu/~dennis/MatrixCalculus.pdf\n",
    "    - https://math.stackexchange.com/questions/312077/differentiate-fx-xtax\n",
    "- 注意识别和区分一些概念：\n",
    "    - 从线性代数到矩阵分析：就是多了矩阵微积分的操作；\n",
    "    - 标量/矢量关于矢量/矩阵的微分\n",
    "        - 标量关于矢量\n",
    "        - 矢量关于矢量\n",
    "        - **标量关于矩阵**\n",
    "        - **矢量关于矩阵**\n",
    "    - scalar (loss) w.r.t sth 的导数，其 shape 就与 sth 的 shape 一致；\n",
    "- 偏数学的推导及证明还是要回到代码，回到 torch 中的应用，尤其是 `loss.backward`\n",
    "    - 数学证明里边一个很重要的技巧就是等价替换，也复用已知结论；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65850e36-1eae-4352-939a-ca651282bde4",
   "metadata": {},
   "source": [
    "## $\\mathbf y=\\mathbf {Ax}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493ffd5-d2ad-4839-8350-b97e322f7ada",
   "metadata": {},
   "source": [
    "- $\\mathbf x\\in \\mathbb R^n, \\mathbf A\\in \\mathbb R^{m\\times n} \\rightarrow \\mathbf y\\in \\mathbb R^{m}$\n",
    "- 线性变换的角度就是 $\\mathbb R^n\\rightarrow \\mathbb R^m$ 的映射/投影（project）\n",
    "    - transformer 中的 ffn（h -> 4h -> h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d924ef-2549-4345-b659-f56167cd246e",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{y} = \\psi(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "- $\\mathbf{y} = \\psi(\\mathbf{x}),$ 比如 $\\mathbf y=\\mathbf {Ax}$\n",
    "- $\\frac{\\partial \\mathbf y}{\\partial \\mathbf x}$ 向量（多元输出，multi-variables）对向量（多元输入，multi-inputs）的导数，此时的 gradient 是 jacobian matrix\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\mathbf y=\\mathbf {Ax}\\\\\n",
    "&\\frac{\\partial \\mathbf y}{\\partial \\mathbf x}=\\mathbf A\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- 我们来进行简单的推导\n",
    "    - $y_i=A_{[i]}x=\\sum_k a_{ik}x_k$\n",
    "        - $y_1=\\sum_ka_{1k}x_k$\n",
    "- 一个特例，当 $A$ 为一个行向量时（$\\mathbf w^T$），退化为一个多元输入，单输出（标量 scalar 输出）的内积运算，此时的导数为与输入等shape的向量；\n",
    "\n",
    "    $$\n",
    "    y=\\mathbf w^T\\mathbf x\n",
    "    $$\n",
    "  \n",
    "    - $y=\\mathbf w^T\\mathbf x=\\sum_iw_ix_i$\n",
    " \n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\mathbf x}=\\begin{bmatrix}w_1,w_2,\\cdots,w_n\\end{bmatrix}^T=\\mathbf w\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47b3d3c7-98b9-4dd7-a7a8-5c5279417268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T08:50:52.726478Z",
     "iopub.status.busy": "2024-07-27T08:50:52.725832Z",
     "iopub.status.idle": "2024-07-27T08:50:52.744958Z",
     "shell.execute_reply": "2024-07-27T08:50:52.742952Z",
     "shell.execute_reply.started": "2024-07-27T08:50:52.726430Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义矩阵 A 和向量 x\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[0.5], [1.0]], requires_grad=True)\n",
    "\n",
    "# 计算 y = A * x\n",
    "y = torch.matmul(A, x)\n",
    "\n",
    "# 初始化雅可比矩阵\n",
    "jacobian = torch.zeros_like(A)\n",
    "\n",
    "# 计算每个 y 的元素关于 x 的导数\n",
    "for i in range(y.size(0)):\n",
    "    y[i].backward(retain_graph=True)\n",
    "    jacobian[i] = x.grad.view(1, -1)\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4be69-15bd-4586-8881-85a296582332",
   "metadata": {},
   "source": [
    "### $\\mathbf y=\\mathbf x \\mathbf W$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defa0f49-b981-4619-8e3d-c91b8f96e700",
   "metadata": {},
   "source": [
    "- $\\mathbf x$ 是一个行向量\n",
    "- 对原式做等价替换：$\\mathbf y^T=\\mathbf W^T\\mathbf x^T$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf y}{\\partial \\mathbf x}=\\frac{\\partial \\mathbf y^T}{\\partial \\mathbf x^T}=\\mathbf W^T\n",
    "$$\n",
    "\n",
    "构造一个简单的示例辅助理解：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\begin{pmatrix}y_1 & y_2 & y_3\\end{pmatrix}&=\\begin{pmatrix}x_1 & x_2\\end{pmatrix}\\begin{pmatrix}w_{11} & w_{12} & w_{13}\\\\\n",
    "w_{21} & w_{22} & w_{23}\n",
    "\\end{pmatrix}\\\\\n",
    "&=\\begin{pmatrix}w_{11}x_1+w_{21}x_2 & w_{12}x_1+w_{22}x_2 & w_{13}x_1+w_{23}x_2\\end{pmatrix}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\\frac{\\partial y_1}{\\partial \\mathbf x} & \\frac{\\partial y_2}{\\partial \\mathbf x} & \\frac{\\partial y_3}{\\partial \\mathbf x}\\end{pmatrix}=\\begin{pmatrix}w_{11} & w_{21}\\\\\n",
    "w_{12} & w_{22}\\\\\n",
    "w_{13} & w_{23}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4ca8173-c53e-4e5b-ba75-a6f5916d7c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T08:53:00.677140Z",
     "iopub.status.busy": "2024-07-27T08:53:00.676514Z",
     "iopub.status.idle": "2024-07-27T08:53:00.694262Z",
     "shell.execute_reply": "2024-07-27T08:53:00.692201Z",
     "shell.execute_reply.started": "2024-07-27T08:53:00.677094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3., 5.],\n",
      "        [2., 4., 6.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义矩阵 A 和向量 x\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[0.5, 1.0, 2]], requires_grad=True)\n",
    "\n",
    "# 计算 y = A * x\n",
    "y = torch.matmul(x, A)[0]\n",
    "\n",
    "# 初始化雅可比矩阵\n",
    "jacobian = torch.zeros(2, 3)\n",
    "\n",
    "# 计算每个 y 的元素关于 x 的导数\n",
    "for i in range(y.size(0)):\n",
    "    y[i].backward(retain_graph=True)\n",
    "    jacobian[i] = x.grad.view(1, -1)\n",
    "    x.grad.zero_()\n",
    "\n",
    "print(jacobian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a02798-1f75-4ee3-b903-111096ce3d2a",
   "metadata": {},
   "source": [
    "## $\\alpha=\\mathbf y^T\\mathbf A\\mathbf x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6a1cf2-9983-4956-a823-992475d7accf",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "&\\frac{\\partial \\alpha}{\\partial \\mathbf x}=\\mathbf y^T\\mathbf A\\\\\n",
    "&\\frac{\\partial \\alpha}{\\partial \\mathbf y}=\\mathbf x^T\\mathbf A^T\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "来看证明：\n",
    "- 对于第一个导数\n",
    "    - $\\mathbf w^T=\\mathbf y^T\\mathbf A$\n",
    "    - $\\alpha=\\mathbf w^T\\mathbf x$\n",
    "    - $\\frac{\\partial \\alpha}{\\partial \\mathbf x}=\\mathbf w^T=\\mathbf y^T\\mathbf A$\n",
    "- 对于第二个导数\n",
    "    - $\\alpha=\\alpha^T=\\mathbf x^T\\mathbf A^T\\mathbf y$\n",
    "    - $\\frac{\\partial \\alpha}{\\partial \\mathbf y}=\\mathbf x^T\\mathbf A^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b4e24-a4c8-416e-bf9b-7fc884f1bcb0",
   "metadata": {},
   "source": [
    "## $\\alpha =\\mathbf x^T\\mathbf A\\mathbf x$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c3f24-a6a4-4534-ab4c-f0163dbe2350",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\alpha}{\\partial \\mathbf x}=(\\mathbf A+\\mathbf A^T)\\mathbf x\n",
    "$$\n",
    "\n",
    "证明，基于矩阵矢量乘法的定义/计算：\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "&\\alpha=\\sum_ix_i\\sum_ja_{ij}x_j=\\sum_i\\sum_jx_ia_{ij}x_j\\\\\n",
    "&\\frac{\\partial \\alpha}{\\partial x_k}=\\sum_ix_ia_{ik}+\\sum_jx_ka_{kj}\\\\\n",
    "&\\frac{\\partial \\alpha}{\\partial \\mathbf x}=\\mathbf A^T\\mathbf x+\\mathbf A\\mathbf x=(\\mathbf A+\\mathbf A^T)\\mathbf x\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "一些特例：\n",
    "\n",
    "- $\\mathbf A^T=\\mathbf A$ 时，$\\frac{\\partial \\alpha}{\\partial \\mathbf A}=2\\mathbf A\\mathbf x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dbe46a7d-da58-41ad-a28e-2aee82626130",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T08:59:37.575790Z",
     "iopub.status.busy": "2024-07-27T08:59:37.575160Z",
     "iopub.status.idle": "2024-07-27T08:59:37.591260Z",
     "shell.execute_reply": "2024-07-27T08:59:37.589166Z",
     "shell.execute_reply.started": "2024-07-27T08:59:37.575744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(3, 1, requires_grad=True)\n",
    "A = torch.randn(3, 3, requires_grad=True)\n",
    "y = (x.T @ A) @ x\n",
    "y.backward()\n",
    "torch.allclose(x.grad, (A + A.T) @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e3f441-f743-4960-9717-5787985c7219",
   "metadata": {},
   "source": [
    "## $\\mathbf y=\\mathbf A\\mathbf x$：关于矩阵求导"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21bea0-bdab-4249-b874-1168678a34e6",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial \\mathbf y}{\\partial \\mathbf A}\n",
    "$$\n",
    "\n",
    "- 是一个三维的tensor\n",
    "    - $\\frac{\\partial y_i}{\\partial \\mathbf A}$ 各是一个矩阵\n",
    "    - $y_1=w_{11}x_1+w_{12}x_2$ => $\\begin{bmatrix}x_1 & x_2\\\\0 & 0\\end{bmatrix}$\n",
    "    - $y_2=w_{21}x_1+w_{22}x_2$ => $\\begin{bmatrix}0 & 0\\\\x_1 & x_2\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f65dd-a236-48de-a1e4-8e6b39a60f5f",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17d328b8-1ece-4f5c-ac2e-acbc2eea6e01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T10:26:26.668771Z",
     "iopub.status.busy": "2024-07-27T10:26:26.668141Z",
     "iopub.status.idle": "2024-07-27T10:26:26.688538Z",
     "shell.execute_reply": "2024-07-27T10:26:26.686381Z",
     "shell.execute_reply.started": "2024-07-27T10:26:26.668724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [0.5000, 1.0000]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义矩阵 A 和向量 x\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  # [5.0, 6.0]\n",
    "                 ], \n",
    "                 requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[0.5], [1.0]], requires_grad=True)\n",
    "\n",
    "# 计算 y = A * x\n",
    "y = torch.matmul(A, x)\n",
    "print(y.shape)\n",
    "# 计算 y 对 A 的雅可比矩阵\n",
    "# v^T·J\n",
    "y.backward(torch.ones_like(y))\n",
    "\n",
    "# 获取雅可比矩阵\n",
    "jacobian = A.grad\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee1d2dde-253e-477a-aa48-606b68a0d6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T10:27:41.164156Z",
     "iopub.status.busy": "2024-07-27T10:27:41.163534Z",
     "iopub.status.idle": "2024-07-27T10:27:41.184673Z",
     "shell.execute_reply": "2024-07-27T10:27:41.182623Z",
     "shell.execute_reply.started": "2024-07-27T10:27:41.164111Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5000, 1.0000],\n",
       "         [0.0000, 0.0000]],\n",
       "\n",
       "        [[0.0000, 0.0000],\n",
       "         [0.5000, 1.0000]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 定义矩阵 A 和向量 x\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]\n",
    "                 ], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[0.5], [1.0]], requires_grad=True)\n",
    "\n",
    "# 计算 y = A * x\n",
    "y = torch.matmul(A, x)\n",
    "\n",
    "# 初始化一个与 A 形状相同的零张量来存储雅可比矩阵\n",
    "jacobian = torch.zeros((y.size(0), A.size(0), A.size(1)))\n",
    "\n",
    "# 逐元素计算雅可比矩阵\n",
    "for i in range(y.size(0)):\n",
    "    # 清除梯度\n",
    "    A.grad = None\n",
    "    \n",
    "    # 对 y 中的第 i 个元素进行反向传播\n",
    "    y[i].backward(retain_graph=True)\n",
    "    \n",
    "    # 将计算得到的梯度存储在雅可比矩阵中\n",
    "    jacobian[i] = A.grad\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e57cdb87-0888-4525-b788-446112b75350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T16:11:29.336410Z",
     "iopub.status.busy": "2024-07-25T16:11:29.335817Z",
     "iopub.status.idle": "2024-07-25T16:11:29.356159Z",
     "shell.execute_reply": "2024-07-25T16:11:29.354664Z",
     "shell.execute_reply.started": "2024-07-25T16:11:29.336363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.0000, 2.0000],\n",
       "        [1.5000, 3.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义矩阵 A 和向量 x\n",
    "A = torch.tensor([[1.0, 2.0],\n",
    "                  [3.0, 4.0],\n",
    "                  [5.0, 6.0]], requires_grad=True)\n",
    "\n",
    "x = torch.tensor([[0.5], [1.0]], requires_grad=True)\n",
    "\n",
    "# 计算 y = A * x\n",
    "y = torch.matmul(A, x)\n",
    "print(y.shape)\n",
    "# 计算 y 对 A 的雅可比矩阵\n",
    "# v^T·J\n",
    "y.backward(torch.tensor([1., 2., 3.]).view(-1, 1))\n",
    "\n",
    "# 获取雅可比矩阵\n",
    "jacobian = A.grad\n",
    "jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e810d22f-4c71-4c10-9086-a0286f859dbc",
   "metadata": {},
   "source": [
    "## loss backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23cf6f47-7610-480c-8493-fb78f7f8a88e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T15:39:50.734127Z",
     "iopub.status.busy": "2024-07-25T15:39:50.733453Z",
     "iopub.status.idle": "2024-07-25T15:39:51.997240Z",
     "shell.execute_reply": "2024-07-25T15:39:51.996772Z",
     "shell.execute_reply.started": "2024-07-25T15:39:50.734077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL/dx: tensor([[0.0000, 0.5000]])\n",
      "dL/dW: tensor([[1., 1.],\n",
      "        [2., 2.]])\n",
      "dL/db: tensor([[1., 1.]])\n",
      "Manual dL/dW: tensor([[1., 1.],\n",
      "        [2., 2.]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 设定输入 x 和权重 W，b 为偏置\n",
    "x = torch.tensor([[1.0, 2.0]], requires_grad=True)  # 1x2 行向量\n",
    "W = torch.tensor([[0.5, -0.5], [1.5, -1.0]], requires_grad=True)  # 2x2 矩阵\n",
    "b = torch.tensor([[0.1, -0.1]], requires_grad=True)  # 1x2 行向量\n",
    "\n",
    "# 前向传播计算 z = xW + b\n",
    "z = x @ W + b  # 矩阵乘法加上偏置\n",
    "\n",
    "# 定义一个简单的标量损失函数，假设为 z 的和\n",
    "L = z.sum()\n",
    "\n",
    "# 进行反向传播计算梯度\n",
    "L.backward()\n",
    "\n",
    "# 打印梯度\n",
    "print(\"dL/dx:\", x.grad)\n",
    "print(\"dL/dW:\", W.grad)\n",
    "print(\"dL/db:\", b.grad)\n",
    "\n",
    "# 手动验证梯度计算\n",
    "dL_dz = torch.ones_like(z)  # 因为 L = z.sum(), dL/dz = 1\n",
    "dz_dW = x.t()  # d(xW+b)/dW = x^T\n",
    "manual_dL_dW = dz_dW @ dL_dz  # outer product\n",
    "print(\"Manual dL/dW:\", manual_dL_dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbdbce9-0fb9-4394-a0f1-6257d82a5f38",
   "metadata": {},
   "source": [
    "### Learning from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f7c9847-bdff-452a-bad0-18798d90aace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T09:39:17.850820Z",
     "iopub.status.busy": "2024-07-27T09:39:17.850206Z",
     "iopub.status.idle": "2024-07-27T09:39:17.859458Z",
     "shell.execute_reply": "2024-07-27T09:39:17.857368Z",
     "shell.execute_reply.started": "2024-07-27T09:39:17.850771Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5101c38-868a-479f-bfa1-15fb63e58dee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T09:40:50.480778Z",
     "iopub.status.busy": "2024-07-27T09:40:50.480209Z",
     "iopub.status.idle": "2024-07-27T09:40:50.492925Z",
     "shell.execute_reply": "2024-07-27T09:40:50.490849Z",
     "shell.execute_reply.started": "2024-07-27T09:40:50.480731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/s34917042.jpg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='../../imgs/s34917042.jpg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb740c-3a15-449f-adb6-3d72408785b3",
   "metadata": {},
   "source": [
    "- 这本书包括 Strang 老师的 MIT 公开课，陪伴我读研以及工作直至今天；\n",
    "- 整本书非常的凝练，有的放矢，面向现代的AI计算及应用；\n",
    "    - learning from data （data-driven 的方式从数据中进行机器学习/深度学习/AI训练等）的观念就已经非常非常的现代了；\n",
    "- 后边会做一期单独的视频导读这本书；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
